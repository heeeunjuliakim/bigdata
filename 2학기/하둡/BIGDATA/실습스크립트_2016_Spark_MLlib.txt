
Putty로 192.168.56.101에 접속
# id : root
# password : hadoop

$ echo 3 > /proc/sys/vm/drop_caches

$ su  -  hadoop

$ export SPARK_HOME=/data/spark-1.6.0 
$ export SPARK_CONF_DIR=$SPARK_HOME/conf

$ cd $SPARK_HOME
$ whoami
$ pwd


1.Python 언어로 테스트

$ bin/pyspark --master local[2]


>>> lines = sc.textFile("README.md")  

>>> lines.count()  
95

>>> lines.first() 
u'# Apache Spark

>>> pythonLines = lines.filter(lambda line: "Python" in line)

>>> pythonLines.count()
3

>>> CTRL+D


2.Scala 언어로 테스트


$ bin/spark-shell


scala> val x= sc.parallelize(Array("b", "a", "c"))
scala> val y= x.map(z => (z,1))
scala> println(x.collect().mkString(", "))
scala> println(y.collect().mkString(", "))

scala> val x1= sc.parallelize(Array("b", "a", "c"),2)
scala> println(x1.collect().mkString(", "))

scala> CTRL+D



----------------------------------------------------------

Spark 서비스 구동

----------------------------------------------------------


$ jps

$ sbin/start-all.sh

$ jps

웹브라우저에서 http://192.168.56.101:8080/

->  spark://namenode:7077

$ bin/spark-shell --master spark://namenode:7077 


----------------------------------------------------------

Spark Example

----------------------------------------------------------

# NaiveBayes 분류기

scala> import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}
 

scala> import org.apache.spark.mllib.linalg.Vectors
 

scala> import org.apache.spark.mllib.regression.LabeledPoint
 

scala> val data = sc.textFile("file:///data/spark-1.6.0/data/mllib/sample_naive_bayes_data.txt")
 
scala> println(data.collect().mkString("\n"))

scala> val parsedData = data.map { line =>
     |   val parts = line.split(',')
     |   LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))
     | }
  println(parsedData.collect().mkString("\n"))
scala> val splits = parsedData.randomSplit(Array(0.6, 0.4), seed = 11L)
 
scala> val training = splits(0)
 
scala> val test = splits(1)

scala> training.count()



scala> val model = NaiveBayes.train(training, lambda = 1.0, modelType = "multinomial")
 
scala> val predictionAndLabel = test.map(p => (model.predict(p.features), p.label))
 
scala> val accuracy = 1.0 * predictionAndLabel.filter(x => x._1 == x._2).count()
 
scala> test.count()

scala> println(predictionAndLabel.collect().mkString("\n"))

scala> val trainPrediction = training.map(p => (model.predict(p.features), p.label))

scala> val trainAccuracy = 1.0 * trainPrediction.filter(x => x._1 == x._2).count()

scala> training.count()

scala> println(training.collect().mkString("\n"))

scala> println(trainPrediction.collect().mkString("\n"))

scala> model.save(sc, "myModel_NaiveBayes")

scala> val NBModel = NaiveBayesModel.load(sc, "myModel_NaiveBayes")
 

# DecisionTree 분류기

scala> import org.apache.spark.mllib.tree.DecisionTree
 

scala> import org.apache.spark.mllib.tree.model.DecisionTreeModel
 

scala> import org.apache.spark.mllib.util.MLUtils
 

scala> val data = MLUtils.loadLibSVMFile(sc, "file:///data/spark-1.6.0/data/mllib/sample_libsvm_data.txt")

scala> println(data.collect().mkString("\n")) 

scala> val numClasses = 2
 

scala> val categoricalFeaturesInfo = Map[Int, Int]()
 

scala> val impurity = "gini"
 

scala> val maxDepth = 5
 

scala> val maxBins = 32


scala> val splits = data.randomSplit(Array(0.7, 0.3))
 
scala> val (trainingData, testData) = (splits(0), splits(1))


 
scala> val model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo, impurity, maxDepth, maxBins)
 

scala> val labelAndPreds = testData.map { point =>
     | val prediction = model.predict(point.features)
     | (point.label, prediction)
     | }
 

scala> val testErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble

scala> println(labelAndPreds.collect().mkString("\n")) 

scala> testData.count()
 
scala> model.save(sc, "myModel_DTree")
 
scala> val DTModel = DecisionTreeModel.load(sc, "myModel_DTree")


----------------------------------------------------------

Spark를 이용한 머신러닝 실습

----------------------------------------------------------


1.Data Model

# Spark Basic Type

import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.mllib.linalg.{BLAS,Vector,Vectors,Matrix}

-- Create a dense vector (1.0, 0.0, 3.0). 
val dv: Vector = Vectors.dense(1.0, 0.0, 3.0)

-- Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0)) 
val dm: Matrix = Matrices.dense(3, 2, Array(1.0, 3.0, 5.0, 2.0, 4.0, 6.0))

-- Create a array ("Zara", "Nuha", "Ayan")
var z = new Array[String](3);   z(0) = "Zara";   z(1) = "Nuha";   z(2) = "Ayan"
var z2 = Array("Zara", "Nuha", "Ayan")




# Vector Type

-- Create the dense vector <1.0, 0.0, 2.0, 0.0>; 
val denseVec1 = Vectors.dense(1.0, 0.0, 2.0, 0.0)
val denseVec2 = Vectors.dense(Array(1.0, 0.0, 2.0, 0.0))

-- Create the sparse vector <1.0, 0.0, 2.0, 0.0>; 
val sparseVec1 = Vectors.sparse(4, Array(0, 2), Array(1.0, 2.0))

scala> denseVec1.
scala> sparseVec1.
apply           argmax          asInstanceOf    compressed      copy            
foreachActive   isInstanceOf    numActives      numNonzeros     size            
toArray         toDense         toJson          toSparse        toString 

scala> sparseVec1.apply(0)
res7: Double = 1.0

scala> sparseVec1.apply(1)
res8: Double = 0.0

scala> sparseVec1.size
res10: Int = 4




# LabeledPoint Type


scala> val examples = MLUtils.loadLibSVMFile(sc, 
           "file:///data/spark-1.6.0/data/mllib/sample_binary_classification_data.txt")
examples: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]

scala> println(examples.take(2).mkString("\n")) 

(0.0,  (692,  [127,128,129, ... ,656,657], [ 51.0,159.0,253.0, ... ,141.0, 37.0] ) )
(1.0,  (692,  [158,159,160, ... ,682,683], [124.0,253.0,255.0, ... ,253.0,220.0] ) )

scala> val data = sc.textFile("file:///data/spark-1.6.0/data/mllib/sample_binary_classification_data.txt")
scala> println(data.take(2).mkString("\n"))

0 128:51  129:159 130:253 ... 657:141 658:37
1 159:124 160:253 161:255 ... 683:253 684:220



2.데이터 처리

# Spark Example DataSet

$ cd $SPARK_HOME
$ cd data/mllib
$ wc *
   2000    4000   63973 gmm_data.txt
      6      18      72 kmeans_data.txt
   1000   11000  197105 lr_data.txt
      6      12      24 pagerank_data.txt
     19      57     164 pic_data.txt
    100   13610  104736 sample_binary_classification_data.txt
      6      34      68 sample_fpgrowth.txt
     99     100    1598 sample_isotonic_regression_data.txt
     12     132     264 sample_lda_data.txt
    100   13610  104736 sample_libsvm_data.txt
    501    5511  119069 sample_linear_regression_data.txt
   1501    1501   14351 sample_movielens_data.txt
    150     737    6953 sample_multiclass_classification_data.txt
     11      36      95 sample_naive_bayes_data.txt
    322    5474   39474 sample_svm_data.txt
    569     569  115476 sample_tree_data.csv
 
$ wc lr-data/*
 1000  2000 42060 lr-data/random.data
$ wc ridge-data/*
   67   536 10395 ridge-data/lpsa.data



# Data Loading

val data = sc.textFile("file:///data/spark-1.6.0/data/mllib/pic_data.txt")

println(data.take(3).mkString("\n"))
data.first()
data.count()



# Split Column

val initData = data.map { line =>
  val values = line.split(' ').map(_.toDouble)
  Vectors.dense(values.init)
}
println(initData.take(3).mkString("\n"))

val lastData = data.map { line =>
  val values = line.split(' ').map(_.toDouble)
  Vectors.dense(values.last)
}
println(lastData.take(3).mkString("\n"))

val firstData = data.map { line =>
  val values = line.split(' ').map(_.toDouble)
  Vectors.dense(values(0))
}
println(firstData.take(3).mkString("\n"))

val tailData = data.map { line =>
  val values = line.split(' ').map(_.toDouble)
  Vectors.dense(values.tail)
}
println(tailData.take(3).mkString("\n"))



# Data Search

-- RDD stats

val second = data.map { line =>
  val values = line.split(' ').map(_.toDouble)
  (values(1))
}

println(second.collect().mkString("\n"))
second.stats

-- RDD[Vector] stats

val features = data.map { line =>
  val values = line.split(' ').map(_.toDouble)
  Vectors.dense(values)
}
println(features.collect().mkString("\n"))

import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}

val summary: MultivariateStatisticalSummary = Statistics.colStats(features)

summary.mean
summary.variance
summary.numNonzeros

-- RDD[Vector] Correlations  

val correlMatrix: Matrix = Statistics.corr(features, "pearson")


# Data Transform

-- TF(Term Frequency)

[NOTE] http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz
 
import org.apache.spark.mllib.feature.HashingTF

val path = "file:///data/spark-1.6.0/data/train.electronics/*"
val data = sc.wholeTextFiles(path)
val text = data.map { case (file, text) => text }
text.count
text.first()

val tf = new HashingTF(numFeatures = 10000)
val termF = text.map(txt => tf.transform(txt.split(" ")))
termF.first()

res5: org.apache.spark.mllib.linalg.Vector = 
(10000
,[0,22,43,45,54,73,145,182,334,434,866,1015,1075,1563,1577,1666,1691,1786,1818,2266,2284,2752,2932,3159,3355,3480,3502,3521,3524,3543,4145,4262,4304,4468,4773,4801,5240,5475,5636,5848,5853,5936,6048,6051,6216,6852,6979,6988,7118,7285,7525,7817,7865,8056,8642,8661,8786,9011,9131,9467,9493,9792]
,[7.0,1.0,1.0,7.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]
)


--  StandardScaler


import org.apache.spark.mllib.feature.StandardScaler

val example = sc.textFile("file:///data/spark-1.6.0/data/mllib/ridge-data/lpsa.data")
val data = example.map { line =>
  val parts = line.split(',')
  LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))
}.cache()

val scaler1 = new StandardScaler().fit(data.map(x => x.features))
val scaler2 = new StandardScaler(withMean = true, withStd = true).fit(data.map(x => x.features))

val data1 = data.map(x => LabeledPoint(x.label, scaler1.transform(x.features)))
val data2 = data.map(x => LabeledPoint(x.label, scaler2.transform(Vectors.dense(x.features.toArray))))

val summary: MultivariateStatisticalSummary = Statistics.colStats(data.map(x => x.features))
val summary1: MultivariateStatisticalSummary = Statistics.colStats(data1.map(x => x.features))
val summary2: MultivariateStatisticalSummary = Statistics.colStats(data2.map(x => x.features))

summary.mean
summary1.mean
summary2.mean

summary.variance
summary1.variance
summary2.variance




------------------------------------------------------------------ 

1.Binary Classification

with SVMs, Logistic Regression

------------------------------------------------------------------ 

1) 필요한 클래스 import

import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.mllib.linalg.{BLAS,Vector,Vectors,Matrix}
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

import org.apache.spark.mllib.classification.{LogisticRegressionWithLBFGS, SVMWithSGD}
import org.apache.spark.mllib.optimization.{SquaredL2Updater, L1Updater}


2) 예제 데이터 로딩 및 학습/평가 데이터셋 분리

val examples = MLUtils.loadLibSVMFile(sc, 
    "file:///data/spark-1.6.0/data/mllib/sample_binary_classification_data.txt")

val splits = examples.randomSplit(Array(0.5, 0.5))
val training = splits(0).cache()
val test = splits(1).cache()

val numTraining = training.count().toDouble
val numTest = test.count().toDouble

3) SVM 모델 학습 

val numIterations = 5

-- Default : L2 regularization with the regularization parameter set to 1.0 

val modelSVM2 = SVMWithSGD.train(training, numIterations) 

-- 1 또는 0 의 분류 대신 Score를 반환 즉 임계치를 제거함

modelSVM2.clearThreshold() 

val scoreAndLabels = test.map { point =>
val score = modelSVM2.predict(point.features)
(score, point.label)
}

println(scoreAndLabels.collect().mkString("\n")) 

(392871.0018563218,1.0)
(447179.6074155969,1.0)
(-1449174.32670145,0.0)
...


-- L1 regularization with the regularization parameter set to 0.1 

val svms = new SVMWithSGD()
svms.optimizer.setNumIterations(numIterations).setRegParam(0.1).setUpdater(new L1Updater)
val modelSVM = svms.run(training)


val predictionAndLabels = test.map { case LabeledPoint(label, features) =>
val prediction = modelSVM.predict(features)
(prediction, label)
}

println(predictionAndLabels.collect().mkString("\n")) 

(1.0,1.0)
(1.0,1.0)
(0.0,0.0)
...


4) SVM 모델 평가 

predictionAndLabels.filter(x => x._1 != x._2).count()

numTest

val accuracy = predictionAndLabels.filter(x => x._1 == x._2).count().toDouble / numTest

val metrics = new BinaryClassificationMetrics(predictionAndLabels)

println(s"Test areaUnderPR = ${metrics.areaUnderPR()}.")
println(s"Test areaUnderROC = ${metrics.areaUnderROC()}.")


5) Logistic Regression 모델 학습


val lrs = new LogisticRegressionWithLBFGS()
lrs.optimizer.setNumIterations(numIterations)
val modelLR = lrs.run(training) 

val predictionAndLabels = test.map { case LabeledPoint(label, features) =>
val prediction = modelLR.predict(features)
(prediction, label)
}

println(predictionAndLabels.collect().mkString("\n")) 


6) Logistic Regression 모델 평가

predictionAndLabels.filter(x => x._1 != x._2).count()

numTest

val accuracy = predictionAndLabels.filter(x => x._1 == x._2).count().toDouble / numTest

val metrics = new BinaryClassificationMetrics(predictionAndLabels)

println(s"Test areaUnderPR = ${metrics.areaUnderPR()}.")
println(s"Test areaUnderROC = ${metrics.areaUnderROC()}.")




------------------------------------------------------------------------------------

2.MultiClass Classification

with Naive Bayes, Decision Tree, Logistic Regresssion 

------------------------------------------------------------------ 


1) 필요한 클래스 import

import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.evaluation.MulticlassMetrics
import org.apache.spark.rdd._

import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel


2) 예제 데이터 로딩 및 학습/평가 데이터셋 분리

val rawData = sc.textFile("file:///data/spark-1.6.0/data/mllib/sample_tree_data.csv")
val data = rawData.map { line => 
    val parts = line.split(',').map(_.toDouble)
    LabeledPoint(parts(0), Vectors.dense(parts.tail))
}
println(data.take(4).mkString("\n")) 
println(rawData.take(4).mkString("\n")) 

val Array(training, test) = data.randomSplit(Array(0.5, 0.5))
training.cache()
test.cache()

val numTraining = training.count().toDouble
val numTest = test.count().toDouble


3) Naive Bayes 모델 학습

[Note] Naive Bayes 모델은 속성값이 (-)면 처리할 수 없음

val modelNB = NaiveBayes.train(training, lambda = 1.0, modelType = "multinomial")
 
val predictionAndLabels = test.map(p => (modelNB.predict(p.features), p.label))

println(predictionAndLabels.take(20).mkString("\n")) 


4) Naive Bayes 모델 평가
 
predictionAndLabels.filter(x => x._1 != x._2).count()
val accuracy = predictionAndLabels.filter(x => x._1 == x._2).count().toDouble / numTest

val metrics = new MulticlassMetrics(predictionAndLabels)

println(s"Precision = ${metrics.precision}.")
println(s"Recall = ${metrics.recall}.")

[NOTE] Multiclass Classifier의 경우 Accuracy, Precision, Recall은 모두 동일
      (Sum of all false positives == Sum of all false negatives)
 

5) Naive Bayes 모델 매개변수 튜닝

def trainNBWithParams(input: RDD[LabeledPoint], lambda: Double) = {
    val nb = new NaiveBayes
    nb.setLambda(lambda)
    nb.run(input)
}
val nbResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>
    val modelNB = trainNBWithParams(training, param)
    val predictAndLabels = test.map { point =>
    (modelNB.predict(point.features), point.label)
    }
    val metrics = new MulticlassMetrics(predictAndLabels)
    (s"$param lambda", metrics.precision)
}
nbResults.foreach { case (param, prec) => 
     println(f"$param, Precision = $prec")
}


6) Decision Tree Classifier 모델 학습

[NOTE] Label의 클래스 개수를 반드시 확인해야 함

val numClasses = 2
val categoricalFeaturesInfo = Map[Int, Int]()
val impurity = "gini"
val maxDepth = 5
val maxBins = 32

val modelDT = DecisionTree.trainClassifier(
    training, numClasses, categoricalFeaturesInfo, impurity, maxDepth, maxBins)

val predictionAndLabels = test.map(p => (modelDT.predict(p.features), p.label))

println(predictionAndLabels.take(20).mkString("\n")) 
println("Decision Tree Model \n" + modelDT.toDebugString)

7) Decision Tree Classifier 모델 평가

predictionAndLabels.filter(x => x._1 != x._2).count()
val accuracy = predictionAndLabels.filter(x => x._1 == x._2).count().toDouble / numTest

val metrics = new MulticlassMetrics(predictionAndLabels)
println(s"Precision = ${metrics.precision}.")


8) Decision Tree Classifier 매개변수 튜닝

val evaluations = 
    for (
        impurity <- Array("gini", "entropy"); depth <- Array(5, 15); bins <- Array(32,64)
    ) yield {
        val model = DecisionTree.trainClassifier(
                    training, numClasses, Map[Int,Int](), impurity, depth, bins)
        val predictionsAndLabels = test.map(example =>
            (model.predict(example.features), example.label))
        val accuracy = new MulticlassMetrics(predictionsAndLabels).precision
        ((impurity, depth, bins), accuracy)
    }

evaluations.sortBy(_._2).reverse.foreach(println)

9) Gradient-Boosted Trees(GBTs) 모델 학습 및 평가

[Note] Binary만 가능, Multiclass는 불가능

import org.apache.spark.mllib.tree.GradientBoostedTrees
import org.apache.spark.mllib.tree.configuration.BoostingStrategy
import org.apache.spark.mllib.tree.model.GradientBoostedTreesModel
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics


val boostingStrategy = BoostingStrategy.defaultParams("Classification")
boostingStrategy.numIterations = 3 
boostingStrategy.treeStrategy.numClasses = 2
boostingStrategy.treeStrategy.maxDepth = maxDepth
boostingStrategy.treeStrategy.categoricalFeaturesInfo = categoricalFeaturesInfo

val modelGBT = GradientBoostedTrees.train(training, boostingStrategy)

val predictionAndLabels = test.map(p => (modelGBT.predict(p.features), p.label))
val accuracy = predictionAndLabels.filter(x => x._1 == x._2).count().toDouble / numTest

val metrics = new BinaryClassificationMetrics(predictionAndLabels)

println(s"Test areaUnderPR = ${metrics.areaUnderPR()}.")
println(s"Test areaUnderROC = ${metrics.areaUnderROC()}.")
println("GBTs Model \n" + modelGBT.toDebugString)


10) IRIS(붓꽃 데이터) Multiclass Classifier의 모델 비교


val examples = MLUtils.loadLibSVMFile(sc, 
"file:///data/spark-1.6.0/data/mllib/sample_multiclass_classification_data.txt")
val splits = examples.randomSplit(Array(0.5, 0.5))
val training = splits(0).cache()
val test = splits(1).cache()
println(examples.take(4).mkString("\n")) 

val numTraining = training.count().toDouble
val numTest = test.count().toDouble

val numClasses = 3

# LogisticRegressionWithLBFGS

import org.apache.spark.mllib.classification.{LogisticRegressionWithLBFGS, LogisticRegressionModel}

val modelLR = new LogisticRegressionWithLBFGS().setNumClasses(numClasses).run(training)

val predictionsAndLabels = test.map(p =>(modelLR.predict(p.features), p.label))
predictionsAndLabels.filter(x => x._1 != x._2).count()

val metrics = new MulticlassMetrics(predictionsAndLabels)
println(s"Precision = ${metrics.precision}.")

(0 until numClasses).foreach { index =>
  val label = index.toDouble
  println("Label("+label+") Precision:"+metrics.precision(label))
}


# Decision Tree Classifier

val categoricalFeaturesInfo = Map[Int, Int]()
val impurity = "gini"
val maxDepth = 5
val maxBins = 32

val modelDT = DecisionTree.trainClassifier(training, numClasses, 
    categoricalFeaturesInfo, impurity, maxDepth, maxBins)

val predictionsAndLabels = test.map(p => (modelDT.predict(p.features), p.label))
predictionsAndLabels.filter(x => x._1 != x._2).count()

val metrics = new MulticlassMetrics(predictionsAndLabels)
println(s"Precision = ${metrics.precision}.")

(0 until numClasses).foreach { index =>
  val label = index.toDouble
  println("Label("+label+") Precision:"+metrics.precision(label))
}


# Random Forest Classifier

import org.apache.spark.mllib.tree.RandomForest
import org.apache.spark.mllib.tree.model.RandomForestModel

// numClasses, categoricalFeaturesInfo, impurity, maxDepth, maxBins
val numTrees = 3 			// Use more in practice.
val featureSubsetStrategy = "auto" 	// Let the algorithm choose.

val modelRF = RandomForest.trainClassifier(training, numClasses, 
    categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)

val predictionsAndLabels = test.map(p => (modelRF.predict(p.features), p.label))
predictionsAndLabels.filter(x => x._1 != x._2).count()

val metrics = new MulticlassMetrics(predictionsAndLabels)
println(s"Precision = ${metrics.precision}.")

(0 until numClasses).foreach { index =>
  val label = index.toDouble
  println("Label("+label+") Precision:"+metrics.precision(label))
}

println("Random Forest Model \n" + modelRF.toDebugString)

------------------------------------------------------------------------------------

3.Regression

with Linear Method & Decision Tree

------------------------------------------------------------------ 


1) 개요

- 분류는 Class 찾기, 회귀는 연속형 Value 예측

- Linear Regression
linear least squares : no regularization 
ridge regression : L2 regularization 
Lasso : L1 regularization

- Decision Trees Regression
decision trees 
random forests 
gradient-boosted trees 

2) 예제 데이터셋 로딩

val data = sc.textFile("file:///data/spark-1.6.0/data/mllib/ridge-data/lpsa.data")
val parsedData = data.map { line =>
  val parts = line.split(',')
  LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))
}.cache()

println(parsedData.take(4).mkString("\n")) 


3) Linear Regression 클래스 import 

import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

import org.apache.spark.mllib.regression.LinearRegressionWithSGD
import org.apache.spark.mllib.regression.RidgeRegressionWithSGD
import org.apache.spark.mllib.regression.LassoWithSGD 


4) Linear Least Squares 모델 학습 및 평가

val numIterations = 20
val model = LinearRegressionWithSGD.train(parsedData, numIterations)

val valuesAndPreds = parsedData.map { point =>
  val prediction = model.predict(point.features)
  (point.label, prediction)
}
println(valuesAndPreds.take(10).mkString("\n")) 

val MSE = valuesAndPreds.map{case(v, p) => math.pow((v - p), 2)}.mean()
println("Mean Squared Error = " + MSE)
val MAE = valuesAndPreds.map{case(v, p) => math.abs(v - p)}.mean()
println("Mean Absolue Error = " + MAE)



5) Ridge Regression 모델 학습 및 평가

val model = RidgeRegressionWithSGD.train(parsedData, numIterations)

val valuesAndPreds = parsedData.map { point =>
  val prediction = model.predict(point.features)
  (point.label, prediction)
}
println(valuesAndPreds.take(10).mkString("\n")) 

val MSE = valuesAndPreds.map{case(v, p) => math.pow((v - p), 2)}.mean()
println("Mean Squared Error = " + MSE)
val MAE = valuesAndPreds.map{case(v, p) => math.abs(v - p)}.mean()
println("Mean Absolue Error = " + MAE)



6) Lasso 모델 학습 및 평가

val model = LassoWithSGD.train(parsedData, numIterations)

val valuesAndPreds = parsedData.map { point =>
  val prediction = model.predict(point.features)
  (point.label, prediction)
}
println(valuesAndPreds.take(10).mkString("\n")) 

val MSE = valuesAndPreds.map{case(v, p) => math.pow((v - p), 2)}.mean()
println("Mean Squared Error = " + MSE)
val MAE = valuesAndPreds.map{case(v, p) => math.abs(v - p)}.mean()
println("Mean Absolue Error = " + MAE)



7) Decision Tree Regression 클래스 import 

import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.RandomForest
import org.apache.spark.mllib.tree.GradientBoostedTrees
import org.apache.spark.mllib.tree.configuration.BoostingStrategy
 

8) Decision Tree Regression 모델 학습 및 평가

val categoricalFeaturesInfo = Map[Int, Int]()
val impurity = "variance"
val maxDepth = 5
val maxBins = 32

val model = DecisionTree.trainRegressor(parsedData, 
    categoricalFeaturesInfo, impurity, maxDepth, maxBins)

val valuesAndPreds = parsedData.map { point =>
  val prediction = model.predict(point.features)
  (point.label, prediction)
}
println(valuesAndPreds.take(10).mkString("\n")) 

val MSE = valuesAndPreds.map{case(v, p) => math.pow((v - p), 2)}.mean()
println("Mean Squared Error = " + MSE)
val MAE = valuesAndPreds.map{case(v, p) => math.abs(v - p)}.mean()
println("Mean Absolue Error = " + MAE)

println("Learned Regression : Decision Tree Model \n" + model.toDebugString)



9) Random Forest Regression 모델 학습 및 평가

val categoricalFeaturesInfo = Map[Int, Int]()
val numTrees = 3  
val featureSubsetStrategy = "auto"  
val impurity = "variance"
val maxDepth = 4
val maxBins = 32

val model = RandomForest.trainRegressor(parsedData, categoricalFeaturesInfo,
    numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)

val valuesAndPreds = parsedData.map { point =>
  val prediction = model.predict(point.features)
  (point.label, prediction)
}
println(valuesAndPreds.take(10).mkString("\n")) 

val MSE = valuesAndPreds.map{case(v, p) => math.pow((v - p), 2)}.mean()
println("Mean Squared Error = " + MSE)
val MAE = valuesAndPreds.map{case(v, p) => math.abs(v - p)}.mean()
println("Mean Absolue Error = " + MAE)

println("Learned Regression : Forest Model \n" + model.toDebugString)



10) Gradient-Boosted Trees Regression 모델 학습 및 평가

val boostingStrategy = BoostingStrategy.defaultParams("Regression")
boostingStrategy.numIterations = 3
boostingStrategy.treeStrategy.maxDepth = 5
boostingStrategy.treeStrategy.categoricalFeaturesInfo = Map[Int, Int]()

val model = GradientBoostedTrees.train(parsedData, boostingStrategy)

val valuesAndPreds = parsedData.map { point =>
  val prediction = model.predict(point.features)
  (point.label, prediction)
}
println(valuesAndPreds.take(10).mkString("\n")) 

val MSE = valuesAndPreds.map{case(v, p) => math.pow((v - p), 2)}.mean()
println("Mean Squared Error = " + MSE)
val MAE = valuesAndPreds.map{case(v, p) => math.abs(v - p)}.mean()
println("Mean Absolue Error = " + MAE)

println("Learned Regression : GBTs Model \n" + model.toDebugString)


------------------------------------------------------------------------------------

4.Clustering

with K-means, LDA

------------------------------------------------------------------ 

1) 매개변수

k : 클러스터의 개수
maxIterations : 최대 반복 회수
initializationMode : 초기화 메소드
epsilon : Distance Threshold


2) 예제 데이터셋 로딩

val data = sc.textFile("file:///data/spark-1.6.0/data/mllib/gmm_data.txt")
val parsedData = data.map(s => Vectors.dense(s.trim.split(' ').map(_.toDouble))).cache()
println(parsedData.take(10).mkString("\n")) 

--def parse(line: String): Vector = Vectors.dense(line.trim.split(" ").map(_.toDouble))
--val sampleData = sc.textFile("file:///data/spark-1.6.0/data/mllib/kmeans_data.txt").map(parse).cache()
--println(sampleData.take(5).mkString("\n")) 



3) 필요한 클래스 import

import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.mllib.linalg.{Vector, Vectors}

import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}
import org.apache.spark.mllib.clustering.{GaussianMixture,GaussianMixtureModel}
import org.apache.spark.mllib.clustering.{BisectingKMeans,BisectingKMeansModel}



4) K-Means 모델 학습 및 평가

val numClusters = 4
val numIterations = 20

val km = KMeans.train(parsedData, numClusters, numIterations)

km.predict( parsedData.first )
val clusterAndValues = parsedData.map { p =>
  val cluster = km.predict(p)
  (cluster, p)
}

println(clusterAndValues.take(10).mkString("\n")) 

val WSSSE = km.computeCost(parsedData)
km.clusterCenters.zipWithIndex.foreach { case (center, idx) =>
  println(s"Cluster Center ${idx}: ${center}")
}



5) Gaussian mixture 모델 학습 및 평가

val gmm = new GaussianMixture().setK(numClusters).run(parsedData)

val clusterAndValues = parsedData.map { p =>
  val cluster = gmm.predict(p)
  (cluster, p)
}

println(clusterAndValues.take(10).mkString("\n")) 

for (i <- 0 until gmm.k) {
  println("weight=%f\nmu=%s\nsigma=\n%s\n" format
    (gmm.weights(i), gmm.gaussians(i).mu, gmm.gaussians(i).sigma))
}



6) Bisecting k-means 모델 학습 및 평가

val bkmmodel = new BisectingKMeans().setK(numClusters)
val bkm = bkmmodel.run(parsedData)

val p2 = parsedData.take(100)
val clusterAndValues = p2.map { p =>
  val cluster = bkm.predict(p)
  (cluster, p)
}
println(clusterAndValues.take(10).mkString("\n")) 

val WSSSE = bkm.computeCost(parsedData)
bkm.clusterCenters.zipWithIndex.foreach { case (center, idx) =>
  println(s"Cluster Center ${idx}: ${center}")
}


7) Latent Dirichlet Allocation (LDA) 모델 학습

Topic Model : Text Document

import org.apache.spark.mllib.clustering.{LDA, DistributedLDAModel}

val data = sc.textFile("file:///data/spark-1.6.0/data/mllib/sample_lda_data.txt")
val parsedData = data.map(s => Vectors.dense(s.trim.split(' ').map(_.toDouble)))
val corpus = parsedData.zipWithIndex.map(_.swap).cache()

println(parsedData.collect().mkString("\n")) 
println(corpus.collect().mkString("\n")) 

val ldaModel = new LDA().setK(3).run(corpus)

println("Learned topics (as distributions over vocab of " + ldaModel.vocabSize + " words):")
val topics = ldaModel.topicsMatrix
ldaModel.describeTopics


8) LDA 활용

import org.apache.spark.mllib.linalg.distributed.RowMatrix

ldaModel.topicsMatrix

val docs = new RowMatrix( parsedData )
println(docs.rows.collect().mkString("\n")) 

val topicScores = docs.multiply( topics )
println(topicScores.rows.collect().mkString("\n")) 

9.09030091493296    6.315426752031323   10.594272333035718
6.732769313644597   12.095520753565568  10.171709932789835
2.451804426718      5.539747302589586   4.008448270692414
16.21353757737696   10.123192885453596  13.663269537169441
5.903630584478496   4.800375356052614   14.29599405946889
11.251410310613956  3.1868901202621336  7.56169956912391
11.332105593077014  12.945789821627223  6.722104585295762
2.921839918890249   1.7385606945892644  5.3395993865204865
2.1148746688924964  4.683160517290286   1.2019648138172165
8.17761638406169    9.856603076928325   5.965780539009986
9.939982853307729   16.270252763039004  6.789764383653267

[1.0,2.0,6.0,0.0,2.0,  3.0,1.0,1.0,0.0,0.0,  3.0] 
[1.0,3.0,0.0,1.0,3.0,  0.0,0.0,2.0,0.0,0.0,  1.0] 
[1.0,4.0,1.0,0.0,0.0,  4.0,9.0,0.0,1.0,2.0,  0.0] 
[2.0,1.0,0.0,3.0,0.0,  0.0,5.0,0.0,2.0,3.0,  9.0] 
[3.0,1.0,1.0,9.0,3.0,  0.0,2.0,0.0,0.0,1.0,  3.0] 
[4.0,2.0,0.0,3.0,4.0,  5.0,1.0,1.0,1.0,4.0,  0.0]
[2.0,1.0,0.0,3.0,0.0,  0.0,5.0,0.0,2.0,2.0,  9.0]
[1.0,1.0,1.0,9.0,2.0,  1.0,2.0,0.0,0.0,1.0,  3.0]
[4.0,4.0,0.0,3.0,4.0,  2.0,1.0,3.0,0.0,0.0,  0.0]
[2.0,8.0,2.0,0.0,3.0,  0.0,2.0,0.0,2.0,7.0,  2.0]
[1.0,1.0,1.0,9.0,0.0,  2.0,2.0,0.0,0.0,3.0,  3.0]
[4.0,1.0,0.0,0.0,4.0,  5.0,1.0,3.0,0.0,1.0,  0.0]

[126.90205227521946,  146.4014819529251,   138.69646577185543]
[ 78.99670087776742,   86.87368211855701,  115.12961700367556]
[203.93788161339418,  213.89329261572365,  159.16882577088217]
[248.4369560107661,   305.1933071548417,   187.3697368343922]
[260.7499825651285,   226.65035135878915,  251.59966607608237]
[227.4182146272422,   173.7522025383584,   235.8295828343994]
[240.25933962670442,  295.3367040779134,   181.40395629538222]
[247.91716046139803,  212.406012618936,    223.67682691966596]
[178.14786195533077,  147.75012224887809,  219.10201579579115]
[198.6744978648805,   271.67026794833555,  224.65523418678399]
[263.71654237117843,  225.70535818094953,  214.57809944787206]
[151.24078859816964,  100.51125476953533,  176.24795663229503]


# 수고하셨습니다.


